# DiffuScene 快速上手指南 🏠

## 一、项目概述

**DiffuScene** 是一个基于扩散模型（Diffusion Model）的室内场景生成系统。它可以：
- 🎲 **无条件生成**：从零生成完整的卧室/客厅/餐厅布局
- 📝 **文本条件生成**：根据文字描述生成场景（如"一张床和两个床头柜"）
- 🔧 **场景补全**：给定部分家具，自动补全剩余物品
- 🔄 **场景重排**：给定一组家具，生成合理的摆放布局

### 核心技术
- 使用 **DDPM (Denoising Diffusion Probabilistic Models)** 来生成场景布局
- 场景表示：物体的 **位置、尺寸、角度、类别、形状特征**
- 形状用 **自编码器** 压缩成 32 维向量

---

## 二、代码结构总览

```
DiffuScene/
├── scripts/                    # 训练和生成的主入口脚本
│   ├── train_diffusion.py     # 训练扩散模型
│   ├── generate_diffusion.py  # 生成场景
│   ├── train_objautoencoder.py  # 训练物体形状自编码器
│   └── preprocess_data.py     # 数据预处理
│
├── scene_synthesis/            # 核心代码包
│   ├── networks/              # 模型定义（⭐重点）
│   │   ├── diffusion_scene_layout_ddpm.py  # 扩散模型主类
│   │   ├── denoise_net.py                   # UNet 去噪网络
│   │   ├── diffusion_ddpm.py                # DDPM 扩散过程
│   │   └── __init__.py                       # build_network 入口
│   └── datasets/              # 数据加载和编码
│       └── threed_front_dataset.py  # 3D-FRONT 数据集
│
├── config/                    # 配置文件（⭐重点）
│   ├── uncond/               # 无条件生成配置
│   ├── text/                 # 文本条件生成配置
│   └── rearrange/            # 场景重排配置
│
├── run/                       # 一键运行脚本（⭐推荐入口）
│   ├── train.sh              # 训练
│   ├── generate.sh           # 生成
│   ├── train_text.sh         # 文本条件训练
│   └── train_rearrange.sh    # 重排训练
│
├── 3d_front_processed/        # 预处理好的数据集
├── pretrained/                # 预训练的形状自编码器
└── pretrained_diffusion/      # 预训练的扩散模型
```

---

## 三、快速开始

### 3.1 环境检查

你的 conda 环境是 `diffuscene`，先激活并检查：

```bash
conda activate diffuscene
cd /home/ubuntu/myvdb/DiffuScene
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

### 3.2 最简单的测试：生成卧室场景

使用预训练模型生成场景（无需训练）：

```bash
cd /home/ubuntu/myvdb/DiffuScene/scripts

# 设置变量
CONFIG="../config/uncond/diffusion_bedrooms_instancond_lat32_v.yaml"
WEIGHT_FILE="../pretrained_diffusion/bedrooms_uncond/model_30000"
OUTPUT_DIR="../outputs/test_generation"
FUTURE_PKL="../3d_front_processed/threed_future_model_bedroom.pkl"

# 生成场景
python generate_diffusion.py \
    $CONFIG \
    $OUTPUT_DIR \
    $FUTURE_PKL \
    --weight_file $WEIGHT_FILE \
    --n_sequences 10 \
    --clip_denoised
```

**参数说明：**
- `--n_sequences 10`：生成 10 个场景
- `--clip_denoised`：裁剪噪声（生成更稳定的结果）

生成的结果会保存在 `outputs/test_generation/` 目录。

### 3.3 训练你自己的模型

#### 方式 1：使用一键脚本（推荐）

```bash
cd /home/ubuntu/myvdb/DiffuScene

# 编辑 run/train.sh，修改第 4 行：
# exp_dir="your experiment directory"
# 改为：
# exp_dir="../outputs/my_training"

# 然后运行
bash run/train.sh
```

#### 方式 2：直接调用 Python（更灵活）

```bash
cd /home/ubuntu/myvdb/DiffuScene/scripts

CONFIG="../config/uncond/diffusion_bedrooms_instancond_lat32_v.yaml"
EXP_DIR="../outputs/my_bedroom_model"
EXP_NAME="bedroom_test"

python train_diffusion.py \
    $CONFIG \
    $EXP_DIR \
    --experiment_tag $EXP_NAME \
    --seed 42
```

**训练监控：**
- 模型每 2000 步保存一次（见配置文件 `save_frequency: 2000`）
- 保存位置：`EXP_DIR/EXP_NAME/model_XXXXX`
- 如果使用 `--with_wandb_logger`，可以在 WandB 看可视化

---

## 四、配置文件详解

配置文件控制一切！以 `config/uncond/diffusion_bedrooms_instancond_lat32_v.yaml` 为例：

### 4.1 数据配置

```yaml
data:
    dataset_directory: "../3d_front_processed/bedrooms_objfeats_32_64"  # 数据路径
    encoding_type: "cached_diffusion_cosin_angle_objfeatsnorm_lat32_wocm"  # 数据编码方式
    filter_fn: "threed_front_bedroom"  # 只要卧室
    augmentations: ["fixed_rotations"]  # 数据增强：90度旋转
    room_layout_size: "64,64"  # 房间平面图分辨率
```

**关键点：**
- `encoding_type` 决定输入包含哪些字段（角度是否用 cos/sin、是否包含 objfeats 等）
- 改了 `encoding_type` 后，要同步修改 `network.point_dim`！

### 4.2 网络配置

```yaml
network:
    type: "diffusion_scene_layout_ddpm"  # 固定，使用扩散模型
    net_type: "unet1d"  # 固定，使用 UNet1D 去噪网络
    
    # 输入维度（⭐ 重要！）
    point_dim: 62  # 每个物体的特征维度 = 3(位置) + 3(尺寸) + 2(角度) + 22(类别) + 32(形状)
    class_dim: 22  # 卧室有 22 类物体
    angle_dim: 2   # 角度用 [cos, sin] 表示
    objfeat_dim: 32  # 形状特征 32 维
    
    # 条件设置
    instance_condition: true  # 使用实例条件（每个物体有独立的嵌入）
    learnable_embedding: true  # 实例嵌入可学习
    instance_emb_dim: 128  # 实例嵌入维度
    room_mask_condition: false  # 不使用房间掩码条件
    
    # 扩散参数
    diffusion_kwargs:
        schedule_type: 'linear'  # beta 调度类型
        beta_start: 0.0001
        beta_end: 0.02
        time_num: 1000  # 扩散步数
        model_mean_type: 'v'  # 预测 velocity（v = eps - x0）
        loss_type: 'mse'  # 均方误差损失
        loss_iou: true  # 额外计算 IoU 损失
    
    # UNet 结构
    net_kwargs:
        dim: 512  # 基础通道数
        dim_mults: [1, 1, 1, 1]  # 各层通道倍数
        channels: 62  # 输入通道数（必须 = point_dim）
        instanclass_dim: 128  # 条件拼接维度（= instance_emb_dim）
        seperate_all: true  # 分别编码 bbox/class/objfeats
```

### 4.3 训练配置

```yaml
training:
    epochs: 60000  # 训练轮数（实际是迭代次数）
    steps_per_epoch: 500
    batch_size: 128
    save_frequency: 2000  # 每 2000 步保存模型
    optimizer: Adam
    lr: 0.0002  # 学习率
    lr_step: 10000  # 每 10000 步衰减学习率
    lr_decay: 0.5  # 学习率衰减因子
```

---

## 五、模型改进方向（作业建议）

### 🎯 改进方向 1：改进去噪网络结构

**目标：** 让 UNet 学习更复杂的物体关系

**要动的文件：**
1. `scene_synthesis/networks/denoise_net.py` - 修改 `Unet1D` 类
2. `config/uncond/diffusion_bedrooms_instancond_lat32_v.yaml` - 修改 `net_kwargs`

**具体修改：**

```python
# scene_synthesis/networks/denoise_net.py
# 在 Unet1D 中加入自注意力层

class Unet1D(nn.Module):
    def __init__(
        self,
        dim,
        dim_mults=(1, 2, 4, 8),  # 改这里：增加通道
        channels=3,
        # ... 其他参数
    ):
        # 在 ResnetBlock 后加入 Attention
        self.mid_attn = Attention(mid_dim)  # 需要实现 Attention 类
```

**配置文件同步：**
```yaml
net_kwargs:
    dim: 512
    dim_mults: [1, 2, 4, 8]  # 改成 4 层
    use_self_attention: true  # 新增选项
```

---

### 🎯 改进方向 2：改进扩散调度

**目标：** 使用更先进的 beta schedule

**要动的文件：**
1. `scene_synthesis/networks/diffusion_ddpm.py` - 修改 `get_betas()` 函数
2. `config/uncond/diffusion_bedrooms_instancond_lat32_v.yaml` - 修改 `diffusion_kwargs`

**具体修改：**

```python
# scene_synthesis/networks/diffusion_ddpm.py
def get_betas(schedule_type, beta_start, beta_end, time_num):
    if schedule_type == 'cosine':  # 新增 cosine schedule
        s = 0.008
        steps = time_num + 1
        x = torch.linspace(0, time_num, steps)
        alphas_cumprod = torch.cos(((x / time_num) + s) / (1 + s) * torch.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0, 0.999)
    # ... 原有代码
```

**配置文件：**
```yaml
diffusion_kwargs:
    schedule_type: 'cosine'  # 改这里
```

---

### 🎯 改进方向 3：添加物体关系约束

**目标：** 让生成的家具摆放更合理（例如床头柜应该在床旁边）

**要动的文件：**
1. `scene_synthesis/networks/diffusion_scene_layout_ddpm.py` - 修改 `get_loss()` 方法
2. 新建 `scene_synthesis/networks/relationship_loss.py`

**具体修改：**

```python
# scene_synthesis/networks/relationship_loss.py (新建文件)
import torch
import torch.nn as nn

class RelationshipLoss(nn.Module):
    """惩罚不合理的物体关系"""
    def __init__(self):
        super().__init__()
    
    def forward(self, translations, class_labels):
        # 示例：床头柜应该靠近床
        bed_indices = (class_labels == BED_CLASS_ID)
        nightstand_indices = (class_labels == NIGHTSTAND_CLASS_ID)
        
        if bed_indices.any() and nightstand_indices.any():
            bed_pos = translations[bed_indices]
            nightstand_pos = translations[nightstand_indices]
            # 计算距离，如果太远则惩罚
            dist = torch.cdist(bed_pos, nightstand_pos)
            loss = torch.relu(dist - 2.0).mean()  # 距离超过 2.0 米惩罚
            return loss
        return 0.0
```

然后在 `diffusion_scene_layout_ddpm.py` 的 `get_loss()` 中加入：

```python
# 在 get_loss() 方法中
relation_loss = self.relationship_loss(translations, class_labels)
total_loss = diffusion_loss + 0.1 * relation_loss  # 加权
```

---

### 🎯 改进方向 4：改进物体形状表示

**目标：** 提升形状编码质量

**要动的文件：**
1. `scene_synthesis/networks/foldingnet_autoencoder.py` - 修改自编码器结构
2. `config/obj_autoencoder/bed_living_diningrooms_lat32.yaml` - 修改配置

**注意：** 改了形状编码器后，需要：
1. 重新训练形状自编码器
2. 重新提取所有物体的形状特征
3. 重新预处理数据集

---

### 🎯 改进方向 5：数据增强

**目标：** 增加训练数据多样性

**要动的文件：**
1. `scene_synthesis/datasets/threed_front_dataset.py` - 修改 `ThreedFront` 类

**具体修改：**

```python
# scene_synthesis/datasets/threed_front_dataset.py
class ThreedFront:
    def __getitem__(self, idx):
        # ... 原有代码获取 scene ...
        
        # 新增随机缩放
        if self.training and random.random() < 0.5:
            scale = random.uniform(0.9, 1.1)
            scene['translations'] *= scale
            scene['sizes'] *= scale
        
        # 新增随机抖动
        if self.training:
            scene['translations'] += torch.randn_like(scene['translations']) * 0.01
        
        return scene
```

---

## 六、调试与验证

### 6.1 检查模型是否正常训练

```bash
# 查看损失日志
tail -f outputs/my_training/bedroom_test/log.txt

# 或使用 tensorboard（如果有）
tensorboard --logdir outputs/my_training
```

### 6.2 可视化生成结果

生成的场景会保存为：
- `.pkl` 文件：包含物体位置、尺寸等数据
- `.obj` 文件（如果开启 `--save_mesh`）：3D 模型

可以用 Blender 或 MeshLab 打开 `.obj` 文件查看。

### 6.3 评估指标

```bash
# 计算 FID 分数（评估生成质量）
cd scripts
python compute_fid_scores.py \
    --generated_path ../outputs/test_generation \
    --real_path ../3d_front_processed/bedrooms_objfeats_32_64
```

---

## 七、常见问题与解决

### ❓ 问题 1：加载预训练模型报错 `size mismatch`

**原因：** 修改了 `point_dim` 或 `net_kwargs` 后，模型结构变了

**解决：**
```bash
# 方案 1：从头训练（不加载预训练模型）
python train_diffusion.py $CONFIG $EXP_DIR --experiment_tag $NAME

# 方案 2：修改配置文件恢复原始维度
```

### ❓ 问题 2：显存不足

**解决：**
```yaml
# 修改配置文件中的 batch_size
training:
    batch_size: 64  # 从 128 改为 64
```

### ❓ 问题 3：生成的场景质量差

**可能原因和解决：**
1. **训练不够久** → 继续训练
2. **学习率太大** → 减小 `lr` 从 0.0002 到 0.0001
3. **数据增强太强** → 关闭部分增强
4. **扩散步数太少** → 增加 `time_num` 从 1000 到 2000

### ❓ 问题 4：找不到数据集

**检查：**
```bash
ls -la /home/ubuntu/myvdb/DiffuScene/3d_front_processed/bedrooms_objfeats_32_64/
```

应该看到很多子文件夹和 `dataset_stats.txt`。

---

## 八、文件修改检查清单

改模型前，按这个清单检查：

- [ ] **改了网络结构** → 同步修改 `config/*.yaml` 的 `net_kwargs`
- [ ] **改了输入维度** → 同步修改 `point_dim`, `channels`, `class_dim`, `objfeat_dim`
- [ ] **改了条件类型** → 同步修改 `instance_condition`, `text_condition` 等开关
- [ ] **改了扩散参数** → 修改 `diffusion_kwargs`
- [ ] **改了数据编码** → 修改 `encoding_type` 并确保维度一致
- [ ] **加了新损失** → 在 `get_loss()` 中添加，并在配置中加权重参数

---

## 九、推荐的改进路线

### 阶段 1：熟悉（1-2 天）
1. 运行预训练模型生成场景
2. 阅读 `diffusion_scene_layout_ddpm.py` 理解模型结构
3. 可视化生成结果

### 阶段 2：小改进（3-5 天）
1. 调整超参数（学习率、batch size、扩散步数）
2. 修改 beta schedule（cosine vs linear）
3. 调整 UNet 层数和通道数

### 阶段 3：大改进（1-2 周）
1. 添加自注意力层
2. 设计物体关系约束
3. 改进条件机制（文本、图像等）

### 阶段 4：创新（根据时间）
1. 多房间联合生成
2. 用户交互式编辑
3. 风格迁移

---

## 十、关键代码位置速查表

| 想做什么 | 要看/改的文件 |
|---------|--------------|
| 修改 UNet 结构 | `scene_synthesis/networks/denoise_net.py` |
| 修改扩散过程 | `scene_synthesis/networks/diffusion_ddpm.py` |
| 修改损失函数 | `scene_synthesis/networks/diffusion_scene_layout_ddpm.py` → `get_loss()` |
| 修改条件输入 | `scene_synthesis/networks/diffusion_scene_layout_ddpm.py` → `__init__()` |
| 修改数据编码 | `scene_synthesis/datasets/threed_front_dataset.py` |
| 调整超参数 | `config/uncond/*.yaml` |
| 训练入口 | `scripts/train_diffusion.py` |
| 生成入口 | `scripts/generate_diffusion.py` |

---

## 十一、额外资源

- **论文：** [DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis](https://arxiv.org/abs/2303.14207)
- **项目主页：** https://tangjiapeng.github.io/projects/DiffuScene/
- **原始 README：** `/home/ubuntu/myvdb/DiffuScene/README.md`
- **详细代码注释：** `/home/ubuntu/myvdb/DiffuScene/MODEL_STRUCTURE_AND_MODIFICATION_GUIDE.md`

---

## 十二、快速命令参考

```bash
# 激活环境
conda activate diffuscene
cd /home/ubuntu/myvdb/DiffuScene

# 训练
bash run/train.sh

# 生成
bash run/generate.sh

# 从特定 checkpoint 继续训练
python scripts/train_diffusion.py \
    config/uncond/diffusion_bedrooms_instancond_lat32_v.yaml \
    outputs/my_model \
    --weight_file outputs/my_model/bedroom/model_10000 \
    --continue_from_epoch 10000

# 生成并保存 mesh
python scripts/generate_diffusion.py \
    config/uncond/diffusion_bedrooms_instancond_lat32_v.yaml \
    outputs/generation \
    3d_front_processed/threed_future_model_bedroom.pkl \
    --weight_file pretrained_diffusion/bedrooms_uncond/model_30000 \
    --n_sequences 50 \
    --save_mesh \
    --render_top2down
```

---

**祝你作业顺利！如果有问题，重点检查配置文件和模型维度是否匹配。** 🚀
